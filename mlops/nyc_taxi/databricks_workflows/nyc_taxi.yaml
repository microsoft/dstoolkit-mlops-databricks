custom:
  basic-cluster-props: &basic-cluster-props
    spark_version: "13.0.x-cpu-ml-scala2.12"
    node_type_id: "Standard_DS3_v2"
    spark_env_vars:
      PYSPARK_PYTHON: "/databricks/python3/bin/python3"
    enable_elastic_disk: true
    runtime_engine: STANDARD
    autoscale:
      min_workers: 2
      max_workers: 8


  nyc_taxi_vars: &nyc_taxi_vars
    job_clusters:
      - job_cluster_key: training_job_cluster
        new_cluster:
          <<: *basic-cluster-props                        
    tasks:
      - task_key: "nyc_taxi_pipeline_data_engineering"
        job_cluster_key: "training_job_cluster"
        spark_python_task:
            python_file: "file://dataops/src_nyc_taxi/transform.py" 

      - task_key: "nyc_taxi_pipeline_machine_learning"
        job_cluster_key: "training_job_cluster"
        spark_python_task:
            python_file: "file://src/pkg/nyc_taxi/entrypoint.py"
        depends_on:
          - task_key: "nyc_taxi_pipeline_data_engineering"

build:
  python: poetry

environments:
  default:
    workflows:
      - name: "DatabricksArtifacts"
        <<: *nyc_taxi_vars