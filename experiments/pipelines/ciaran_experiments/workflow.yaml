custom:

  # Cluster configs for each environment
  default-cluster-spec: &default-cluster-spec
    spark_version: '11.3.x-cpu-ml-scala2.12'
    node_type_id: 'Standard_DS3_v2' 
    driver_node_type_id: 'Standard_DS3_v2'  
    num_workers: 1
    # To reduce start up time for each job, it is advisable to use a cluster pool. To do so involves supplying the following
    # two fields with a pool_id to acquire both the driver and instances from.
    # If driver_instance_pool_id and instance_pool_id are set, both node_type_id and driver_node_type_id CANNOT be supplied.
    # As such, if providing a pool_id for driver and worker instances, please ensure that node_type_id and driver_node_type_id are not present
#    driver_instance_pool_id: '0617-151415-bells2-pool-hh7h6tjm'
#    instance_pool_id: '0617-151415-bells2-pool-hh7h6tjm'

  dev-cluster-config: &dev-cluster-config
    new_cluster:
      <<: *default-cluster-spec

  staging-cluster-config: &staging-cluster-config
    new_cluster:
      <<: *default-cluster-spec

  prod-cluster-config: &prod-cluster-config
    new_cluster:
      <<: *default-cluster-spec

#build:
#  no_build: true     
build:
  python: poetry
  #python: "poetry build -f wheel"

environments:
  default:
    workflows:
      - name: NEW_FUNCTION 
        tasks:
          - task_key: "NEW_FUNCTION"
            existing_cluster_id: "0524-153828-e2rk9h52"
            spark_python_task:
              python_file: "{{var['parameters']['file_path']}}"

      - name: FEATURE_TABLE_REFRESH 
        tasks:
          - task_key: "FEATURE_TABLE_REFRESH"
            existing_cluster_id: "0524-153828-e2rk9h52"
            spark_python_task:
              python_file: "{{var['parameters']['file_path']}}"

      - name: MODEL_TRAINING 
        tasks:
          - task_key: "MODEL_TRAINING"
            existing_cluster_id: "0524-153828-e2rk9h52"
            spark_python_task:
              python_file: "{{var['parameters']['file_path']}}"